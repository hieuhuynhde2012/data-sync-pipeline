# Data Synchronization Pipeline ğŸš€

A real-time data synchronization pipeline using **MySQL**, **Kafka**, **Apache Spark**, **MongoDB**, and **Redis**.  
This project showcases how to detect changes in a relational database and reflect them in NoSQL stores and in-memory caches using a modern data architecture.

---

## ğŸ§© Architecture

![Data Sync Pipeline](pipeline.jpg)

---

## ğŸ› ï¸ Tech Stack

- **MySQL**: Main relational database for data input and trigger-based CDC (Change Data Capture)
- **Kafka**: Message broker to stream changes between systems
- **Apache Spark**: For data transformation and delivery
- **MongoDB**: Destination database for query-intensive operations
- **Redis**: In-memory cache for fast access
- **Python**: Used to connect, validate, and trigger data flow

---

## âš™ï¸ Steps in the Pipeline

1. Create schema in MySQL  
   - Define tables, keys, and constraints  
   - Connect via Python  
2. Insert test data into MySQL, MongoDB, and Redis  
   - Spark validation applied  
3. Build MySQL trigger to detect changes  
4. Capture triggered data and push to Kafka  
   - Using producer, topic, and consumer  
   - Spark validates streamed data  
5. Spark consumes data from Kafka  
   - Applies transformations  
   - Writes results to MongoDB and Redis  

---

## ğŸ“ Directory Structure (Suggestion)

<!-- ```
data-sync-pipeline/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ mysql/
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ consumer.py
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ images/
â”‚   â””â”€â”€ pipeline.jpg
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
``` -->

---

## ğŸ“Œ Purpose

This project demonstrates how to synchronize data in real-time across different storage layers using a simple, modular, and extendable architecture. It simulates real-world data engineering pipelines suitable for analytics, caching, and service scaling.

---

## âœ¨ Author

Huynh Trung Hieu - UIT
